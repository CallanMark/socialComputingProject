{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6574067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import UPFD\n",
    "# different feature types can be selected: content(profile + spacy; dim: 310), profile(dim: 10), spacy(dim: 300)\n",
    "# splits: train, test, val\n",
    "# name: politifact, gossipcop\n",
    "dataset = UPFD('data/upfd', name=\"politifact\", feature='bert', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09215d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph at index 0: Data(x=[72, 768], edge_index=[2, 71], y=[1])\n",
      "Node features shape: torch.Size([72, 768])\n",
      "Node labels shape: torch.Size([1])\n",
      "Edge index shape: torch.Size([2, 71])\n",
      "Edge index: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
      "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "         24, 24, 24, 24, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 60],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]])\n"
     ]
    }
   ],
   "source": [
    "graph = dataset[0]\n",
    "print(f\"Graph at index 0: {graph}\")\n",
    "print(f\"Node features shape: {graph.x.shape}\")\n",
    "print(f\"Node labels shape: {graph.y.shape}\")\n",
    "\n",
    "print(f\"Edge index shape: {graph.edge_index.shape}\")\n",
    "print(f\"Edge index: {graph.edge_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d456c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph directed? True\n"
     ]
    }
   ],
   "source": [
    "def is_directed(edge_index):\n",
    "    edge_set = set(map(tuple, edge_index.t().tolist()))\n",
    "    for u, v in edge_set:\n",
    "        if (v, u) not in edge_set:\n",
    "            return True  # missing reverse edge => directed\n",
    "    return False  # all reverse edges found => undirected\n",
    "\n",
    "print(f\"Is graph directed? {is_directed(graph.edge_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77620662",
   "metadata": {},
   "source": [
    "# transform the homogeneous graphs(UFPD) to Heterogeneous graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04fa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData, Data\n",
    "from typing import List, Union\n",
    "\n",
    "def convert_single_graph(homogeneous_graph: Data, source_node_idx: int = 0, add_source_self_loop: bool = False) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Convert a single homogeneous graph to a heterogeneous graph with two node types:\n",
    "    - 'source': News source node\n",
    "    - 'user': All other nodes\n",
    "    \n",
    "    And two edge types:\n",
    "    - ('source', 'to', 'user'): Edges from source to users\n",
    "    - ('user', 'to', 'user'): Edges between users\n",
    "    - ('source', 'to', 'source'): Self-loop for source node (optional)\n",
    "    \n",
    "    Args:\n",
    "        homogeneous_graph: A PyTorch Geometric Data object\n",
    "        source_node_idx: Index of the source node in the graph, default is 0\n",
    "        add_source_self_loop: Whether to add a self-loop to the source node, default is False\n",
    "        \n",
    "    Returns:\n",
    "        A HeteroData object\n",
    "    \"\"\"\n",
    "    hetero_graph = HeteroData()\n",
    "    \n",
    "    # Get total number of nodes\n",
    "    num_nodes = homogeneous_graph.num_nodes\n",
    "    \n",
    "    # Extract features for source node\n",
    "    source_features = homogeneous_graph.x[source_node_idx:source_node_idx+1]\n",
    "    \n",
    "    # Extract features for user nodes (all nodes except source)\n",
    "    user_indices = torch.cat([\n",
    "        torch.arange(0, source_node_idx), \n",
    "        torch.arange(source_node_idx + 1, num_nodes)\n",
    "    ])\n",
    "    user_features = homogeneous_graph.x[user_indices]\n",
    "    \n",
    "    # Add node features to the heterogeneous graph\n",
    "    hetero_graph['source'].x = source_features\n",
    "    hetero_graph['user'].x = user_features\n",
    "    \n",
    "    # Create a mapping from original node indices to new node indices\n",
    "    node_mapping = {}\n",
    "    node_mapping[source_node_idx] = ('source', 0)  # Source node maps to index 0 in 'source' type\n",
    "    \n",
    "    # Map all other nodes to 'user' type\n",
    "    user_counter = 0\n",
    "    for i in range(num_nodes):\n",
    "        if i != source_node_idx:\n",
    "            node_mapping[i] = ('user', user_counter)\n",
    "            user_counter += 1\n",
    "    \n",
    "    # Process edges\n",
    "    edge_index = homogeneous_graph.edge_index\n",
    "    \n",
    "    # Source-to-user edges and User-to-user edges\n",
    "    source_to_user_edges = []\n",
    "    user_to_user_edges = []\n",
    "    \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        \n",
    "        src_type, src_idx = node_mapping[src]\n",
    "        dst_type, dst_idx = node_mapping[dst]\n",
    "        \n",
    "        if src_type == 'source' and dst_type == 'user':\n",
    "            # Source to user edge\n",
    "            source_to_user_edges.append((src_idx, dst_idx))\n",
    "        elif src_type == 'user' and dst_type == 'user':\n",
    "            # User to user edge\n",
    "            user_to_user_edges.append((src_idx, dst_idx))\n",
    "        # We ignore user-to-source edges as mentioned in the requirements\n",
    "    \n",
    "    # Add edges to the heterogeneous graph\n",
    "    if source_to_user_edges:\n",
    "        src_indices, dst_indices = zip(*source_to_user_edges)\n",
    "        hetero_graph['source', 'to', 'user'].edge_index = torch.tensor(\n",
    "            [src_indices, dst_indices], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    if user_to_user_edges:\n",
    "        src_indices, dst_indices = zip(*user_to_user_edges)\n",
    "        hetero_graph['user', 'to', 'user'].edge_index = torch.tensor(\n",
    "            [src_indices, dst_indices], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    # Add self-loop to source node if requested\n",
    "    if add_source_self_loop:\n",
    "        hetero_graph['source', 'to', 'source'].edge_index = torch.tensor(\n",
    "            [[0], [0]], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    # Copy graph-level targets if they exist\n",
    "    if hasattr(homogeneous_graph, 'y'):\n",
    "        hetero_graph['source'].y = homogeneous_graph.y\n",
    "    \n",
    "    return hetero_graph\n",
    "\n",
    "def convert_to_heterogeneous(homogeneous_dataset, source_node_idx=0, add_source_self_loop=False):\n",
    "    \"\"\"\n",
    "    Convert a homogeneous UPFD dataset to a heterogeneous dataset.\n",
    "    \n",
    "    Args:\n",
    "        homogeneous_dataset: A PyTorch Geometric UPFD dataset\n",
    "        source_node_idx: Index of the source node in each graph, default is 0\n",
    "        add_source_self_loop: Whether to add a self-loop to the source node, default is False\n",
    "        \n",
    "    Returns:\n",
    "        A list of HeteroData objects\n",
    "    \"\"\"\n",
    "    # Simply apply convert_single_graph to each graph in the dataset\n",
    "    hetero_dataset = [\n",
    "        convert_single_graph(graph, source_node_idx, add_source_self_loop) \n",
    "        for graph in homogeneous_dataset\n",
    "    ]\n",
    "    \n",
    "    return hetero_dataset\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData, Data\n",
    "from typing import List, Union\n",
    "\n",
    "def convert_single_graph(homogeneous_graph: Data, source_node_idx: int = 0, add_source_self_loop: bool = False) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Convert a single homogeneous graph to a heterogeneous graph with two node types:\n",
    "    - 'source': News source node\n",
    "    - 'user': All other nodes\n",
    "    \n",
    "    And two edge types:\n",
    "    - ('source', 'to', 'user'): Edges from source to users\n",
    "    - ('user', 'to', 'user'): Edges between users\n",
    "    - ('source', 'to', 'source'): Self-loop for source node (optional)\n",
    "    \n",
    "    Args:\n",
    "        homogeneous_graph: A PyTorch Geometric Data object\n",
    "        source_node_idx: Index of the source node in the graph, default is 0\n",
    "        add_source_self_loop: Whether to add a self-loop to the source node, default is False\n",
    "        \n",
    "    Returns:\n",
    "        A HeteroData object\n",
    "    \"\"\"\n",
    "    hetero_graph = HeteroData()\n",
    "    \n",
    "    # Get total number of nodes\n",
    "    num_nodes = homogeneous_graph.num_nodes\n",
    "    \n",
    "    # Extract features for source node\n",
    "    source_features = homogeneous_graph.x[source_node_idx:source_node_idx+1]\n",
    "    \n",
    "    # Extract features for user nodes (all nodes except source)\n",
    "    user_indices = torch.cat([\n",
    "        torch.arange(0, source_node_idx), \n",
    "        torch.arange(source_node_idx + 1, num_nodes)\n",
    "    ])\n",
    "    user_features = homogeneous_graph.x[user_indices]\n",
    "    \n",
    "    # Add node features to the heterogeneous graph\n",
    "    hetero_graph['source'].x = source_features\n",
    "    hetero_graph['user'].x = user_features\n",
    "    \n",
    "    # Create a mapping from original node indices to new node indices\n",
    "    node_mapping = {}\n",
    "    node_mapping[source_node_idx] = ('source', 0)  # Source node maps to index 0 in 'source' type\n",
    "    \n",
    "    # Map all other nodes to 'user' type\n",
    "    user_counter = 0\n",
    "    for i in range(num_nodes):\n",
    "        if i != source_node_idx:\n",
    "            node_mapping[i] = ('user', user_counter)\n",
    "            user_counter += 1\n",
    "    \n",
    "    # Process edges\n",
    "    edge_index = homogeneous_graph.edge_index\n",
    "    \n",
    "    # Source-to-user edges and User-to-user edges\n",
    "    source_to_user_edges = []\n",
    "    user_to_user_edges = []\n",
    "    \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        \n",
    "        src_type, src_idx = node_mapping[src]\n",
    "        dst_type, dst_idx = node_mapping[dst]\n",
    "        \n",
    "        if src_type == 'source' and dst_type == 'user':\n",
    "            # Source to user edge\n",
    "            source_to_user_edges.append((src_idx, dst_idx))\n",
    "        elif src_type == 'user' and dst_type == 'user':\n",
    "            # User to user edge\n",
    "            user_to_user_edges.append((src_idx, dst_idx))\n",
    "        # We ignore user-to-source edges as mentioned in the requirements\n",
    "    \n",
    "    # Add edges to the heterogeneous graph\n",
    "    if source_to_user_edges:\n",
    "        src_indices, dst_indices = zip(*source_to_user_edges)\n",
    "        hetero_graph['source', 'to', 'user'].edge_index = torch.tensor(\n",
    "            [src_indices, dst_indices], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    if user_to_user_edges:\n",
    "        src_indices, dst_indices = zip(*user_to_user_edges)\n",
    "        hetero_graph['user', 'to', 'user'].edge_index = torch.tensor(\n",
    "            [src_indices, dst_indices], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    # Add self-loop to source node if requested\n",
    "    if add_source_self_loop:\n",
    "        hetero_graph['source', 'to', 'source'].edge_index = torch.tensor(\n",
    "            [[0], [0]], dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    # Copy graph-level targets if they exist\n",
    "    if hasattr(homogeneous_graph, 'y'):\n",
    "        hetero_graph['source'].y = homogeneous_graph.y\n",
    "    \n",
    "    return hetero_graph\n",
    "\n",
    "def convert_to_heterogeneous(homogeneous_dataset, source_node_idx=0, add_source_self_loop=False):\n",
    "    \"\"\"\n",
    "    Convert a homogeneous UPFD dataset to a heterogeneous dataset.\n",
    "    \n",
    "    Args:\n",
    "        homogeneous_dataset: A PyTorch Geometric UPFD dataset\n",
    "        source_node_idx: Index of the source node in each graph, default is 0\n",
    "        add_source_self_loop: Whether to add a self-loop to the source node, default is False\n",
    "        \n",
    "    Returns:\n",
    "        A list of HeteroData objects\n",
    "    \"\"\"\n",
    "    # Simply apply convert_single_graph to each graph in the dataset\n",
    "    hetero_dataset = [\n",
    "        convert_single_graph(graph, source_node_idx, add_source_self_loop) \n",
    "        for graph in homogeneous_dataset\n",
    "    ]\n",
    "    \n",
    "    return hetero_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55adb56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  source={\n",
      "    x=[1, 768],\n",
      "    y=[1],\n",
      "  },\n",
      "  user={ x=[71, 768] },\n",
      "  (source, to, user)={ edge_index=[2, 27] },\n",
      "  (user, to, user)={ edge_index=[2, 44] }\n",
      ")\n",
      "Node types: ['source', 'user']\n",
      "Edge types: [('source', 'to', 'user'), ('user', 'to', 'user')]\n",
      "Source features shape: torch.Size([1, 768])\n",
      "User features shape: torch.Size([71, 768])\n",
      "node feature dict {'source': tensor([[ 4.9833e-01,  1.4148e-01,  3.5605e-01, -4.5109e-01, -3.0611e-01,\n",
      "          2.5061e-02,  2.8002e-01, -5.6461e-02, -2.2193e-01, -3.1098e-01,\n",
      "         -3.6787e-01, -5.0690e-02,  9.9056e-02, -2.7402e-01, -2.9179e-01,\n",
      "         -1.3563e-01,  1.1535e-01,  3.4642e-02, -2.8102e-01,  4.0781e-02,\n",
      "         -2.3397e-01, -1.9931e-01,  3.7839e-01, -9.2718e-02,  1.5422e-01,\n",
      "         -1.0526e-01,  7.9626e-02,  2.6651e-02,  3.3124e-01,  2.2782e-01,\n",
      "         -4.6196e-01, -9.3723e-02, -2.3510e-01, -1.8293e-02, -5.5263e-01,\n",
      "         -2.1882e-01,  1.3237e-01,  2.8989e-01, -7.3877e-02, -3.4337e-01,\n",
      "          2.2737e-01,  1.5777e-01, -3.2284e-01, -1.1594e-01, -1.7053e-01,\n",
      "         -3.4415e-01,  4.1308e-01,  5.5251e-01,  4.9701e-02,  2.2024e-01,\n",
      "          6.8173e-01, -1.8886e-01,  2.6792e-01, -3.4116e-01,  6.0337e-02,\n",
      "         -1.7919e-01, -1.0741e-01,  3.4089e-01,  2.6955e-01,  4.6229e-01,\n",
      "         -8.8313e-02, -1.7848e-01,  1.6680e-01, -1.6757e-02, -4.4666e-01,\n",
      "          1.0592e-01, -3.8689e-01, -8.7399e-02,  1.9867e-01, -3.6734e-01,\n",
      "         -2.7909e-02, -5.9345e-02, -2.4496e-01, -2.0377e-02, -1.2686e-01,\n",
      "         -1.9084e-01,  3.7032e-01, -3.9597e-01, -5.1814e-01, -1.3733e-01,\n",
      "          3.8600e-01,  1.5448e-01,  4.9865e-01,  5.7970e-01, -2.2777e-01,\n",
      "         -1.4674e-01,  3.4907e-01, -3.0398e-01,  3.8743e-01,  4.6746e-01,\n",
      "          1.2006e-02, -2.3832e-01,  1.5921e-02, -2.0881e-02,  1.3359e-01,\n",
      "          1.4889e-02,  1.2235e-02, -3.4089e-01, -7.7444e-01,  8.4228e-02,\n",
      "          1.8639e-01, -1.2925e-01,  1.4810e-01,  1.3271e-01,  3.9458e-03,\n",
      "          1.1254e-01,  3.2817e-01, -4.1709e-01, -3.7924e-01, -1.4918e-01,\n",
      "         -2.6106e-01,  2.4664e-01,  2.3349e-01,  1.8836e-01,  5.8085e-01,\n",
      "          2.6960e-01, -2.5443e-01,  2.6301e-01,  9.3538e-02,  4.4720e-01,\n",
      "          4.3324e-02, -1.9146e-01,  1.2773e-01, -2.6602e-02,  6.6994e-01,\n",
      "         -1.1637e-01,  1.7130e-01,  1.1850e-02,  5.2565e-01,  1.6589e-01,\n",
      "         -2.8751e-01,  1.4336e-01,  1.7929e-01, -3.1694e-01,  6.4360e-01,\n",
      "          2.4028e-01,  2.0684e-01,  4.9159e-01, -4.0610e-01, -1.0112e-01,\n",
      "         -2.0397e-01, -1.2868e-01,  6.9020e-02, -1.6788e-01, -1.1398e-01,\n",
      "         -9.6432e-02, -1.3847e-01, -5.1862e-02,  3.6143e-01, -2.2389e-01,\n",
      "         -2.8393e-01, -4.2352e-01,  9.0222e-02, -2.0520e-01, -7.9617e-02,\n",
      "         -2.7483e-01, -2.1710e-01, -8.3181e-02,  3.5027e-01, -4.1755e-01,\n",
      "          2.5407e-01, -1.8569e-01, -7.4766e-02, -5.8946e-01,  5.8267e-02,\n",
      "          1.7584e-02,  3.4543e-01, -9.0783e-02,  2.4536e-01,  3.0861e-01,\n",
      "          2.8589e-01,  1.4864e-01, -5.8605e-01,  6.1834e-01,  3.9532e-02,\n",
      "         -4.5143e-02,  7.2293e-01, -2.1009e-01, -1.4700e-01,  7.8769e-02,\n",
      "         -5.0227e-01,  2.1308e-01,  9.9657e-02,  1.5039e-01,  3.3213e-01,\n",
      "         -8.2904e-03, -7.5821e-01, -2.8153e-01, -1.0798e-01, -1.0624e-01,\n",
      "         -3.1508e-02,  4.7309e-01,  4.3489e-02, -3.2774e-01, -3.9268e-01,\n",
      "         -4.0245e-01,  8.6964e-01,  6.3158e-01, -7.7972e-02, -6.4270e-01,\n",
      "         -3.1103e-01,  2.1047e-01,  3.5432e-01,  8.0300e-02, -3.3451e-01,\n",
      "          9.8429e-03, -4.0907e-02, -9.2478e-02,  1.8670e-01, -2.2843e-01,\n",
      "         -2.2936e-01, -8.7489e-01, -4.4976e-01,  8.7495e-02, -4.7738e-01,\n",
      "         -4.2775e-01, -2.0363e-01, -6.2712e-01,  2.3708e-01,  4.2794e-01,\n",
      "         -1.7671e-01,  2.0321e-01, -1.6231e-02, -5.1056e-01, -1.7538e-01,\n",
      "          2.5584e-01, -5.8265e-02, -1.6288e-02,  5.8066e-01, -3.5122e-01,\n",
      "          2.8434e-01,  9.1404e-01, -1.5739e-01,  2.5894e-02,  2.4057e-01,\n",
      "         -4.1128e-01,  3.0834e-01, -2.7997e-01, -5.7539e-02,  3.1631e-01,\n",
      "         -2.9495e-01,  1.2284e-01,  3.1693e-01, -2.8447e-01,  1.2823e-02,\n",
      "          4.5972e-01, -2.2120e-01,  2.3481e-01, -4.1863e-01, -4.1139e-01,\n",
      "          5.2463e-01,  5.3397e-01, -3.5903e-01, -8.9240e-02,  1.6699e-02,\n",
      "          9.7468e-02,  1.4856e-01,  3.3786e-01,  1.9957e-02, -3.5565e-01,\n",
      "          6.9581e-01, -2.9245e-01, -3.0304e-02, -2.1578e-02, -3.4737e-01,\n",
      "         -9.6716e-01, -3.1507e-02,  2.0279e-01,  3.6645e-02, -2.2937e-01,\n",
      "         -9.8020e-02,  1.2358e-01,  3.3672e-01, -1.3031e-01,  3.0900e-01,\n",
      "          2.2685e-01,  1.7055e-01,  1.7900e-01,  1.2701e-01, -4.6329e-01,\n",
      "          3.3971e-01, -5.0062e-01, -4.4763e-01,  4.4535e-01, -4.7973e-01,\n",
      "         -1.2834e-01, -2.7894e-01,  5.9853e-03,  2.1849e-01,  4.3268e-02,\n",
      "         -1.8651e-01,  2.7100e-01,  3.5050e-01, -1.2949e-01,  9.0714e-02,\n",
      "          7.3053e-02, -7.3582e-01, -1.2262e-02, -4.3288e-01,  8.0753e-02,\n",
      "          2.9504e-01, -2.2287e-01,  4.9008e-01,  3.7230e-01,  3.0273e-01,\n",
      "         -1.1493e-01,  9.6005e-02,  5.0240e-01,  4.2250e-01,  1.5269e-01,\n",
      "          6.5372e-01,  1.1577e-01, -1.5920e-01,  5.1635e-01, -9.6676e-02,\n",
      "         -3.3659e-01, -2.6446e-02, -5.7905e-02, -2.5754e-01,  1.2444e-01,\n",
      "          1.7474e-01, -2.0362e-01, -3.6402e-01,  5.5095e-01,  6.5323e-02,\n",
      "          4.4618e-01, -4.7662e-01,  2.3480e-02,  5.2280e-02,  2.2481e-01,\n",
      "         -2.4435e-01,  3.8269e-02,  5.5869e-01, -3.8977e-01,  2.0469e-01,\n",
      "          5.0389e-02,  3.3900e-01,  8.1077e-02, -1.6302e-01,  6.7280e-02,\n",
      "          4.6966e-01,  2.3239e-01, -1.0848e-01, -3.2918e-01,  5.8506e-01,\n",
      "          4.2029e-04, -1.1313e-02, -4.0298e-01, -4.5129e-01, -4.1209e-01,\n",
      "         -1.2474e+00, -8.1422e-01, -4.4912e-01,  7.3039e-01,  6.2829e-01,\n",
      "          3.1743e-02, -8.8656e-02,  9.3864e-02,  1.2027e-01,  4.2917e-02,\n",
      "          1.2135e-01, -7.7527e-02,  1.0793e-01, -3.6876e-02,  2.0689e-01,\n",
      "         -7.4890e-02,  5.9522e-02,  6.0239e-01, -8.0083e-02, -4.3983e-01,\n",
      "          6.4783e-01, -2.9898e-01,  1.8574e-01,  2.8676e-01,  1.1925e-01,\n",
      "         -1.3693e-01, -6.5880e-02,  2.9003e-01,  4.8045e-01, -2.7925e-01,\n",
      "          1.4648e-04,  2.2738e-01,  1.1611e-01, -1.1670e-01, -2.0747e-01,\n",
      "          4.2796e-01,  6.5658e-02, -5.7038e-01,  3.0141e-01, -2.4589e-01,\n",
      "         -1.2180e-01,  2.4510e-01, -2.8302e-02, -4.6276e-01,  2.2287e-01,\n",
      "         -2.6033e-01, -2.3545e-01,  1.4743e-01, -2.6282e-01,  1.1634e-01,\n",
      "         -5.6174e-01,  7.1446e-02,  4.5546e-02,  1.2560e-02, -7.6813e-01,\n",
      "         -9.7880e-02, -2.2545e-01,  6.0543e-02, -9.9291e-02,  3.1972e-01,\n",
      "         -4.4419e-01, -6.4069e-01,  3.3769e-01,  2.0813e-01,  8.7301e-01,\n",
      "         -4.7873e-01, -3.9192e-01,  1.9400e-01, -2.6025e-01,  3.6889e-02,\n",
      "          3.0272e-01,  3.1199e-01,  1.2644e-01, -4.2758e-02, -2.1478e-02,\n",
      "         -1.1296e-01, -3.1382e-01, -6.3816e-01, -9.2857e-02, -2.2426e-02,\n",
      "          1.4191e-01, -4.6071e-01,  6.1619e-01,  3.2170e-01, -5.0485e-01,\n",
      "         -1.9337e-01,  7.7050e-01, -2.8134e-01,  1.7639e-01, -4.2143e-01,\n",
      "          8.1746e-03, -5.2912e-02,  5.9354e-01, -4.3711e-01,  4.3316e-01,\n",
      "         -9.6088e-02,  9.9395e-02, -5.5046e-02,  6.8160e-02, -3.4798e-01,\n",
      "          3.8596e-01, -8.1237e-02,  2.0905e-01,  2.0193e-01,  7.0137e-02,\n",
      "         -1.1661e+00, -4.7570e-01,  2.2337e-01, -3.7898e-01, -5.3860e-02,\n",
      "         -1.6614e-01, -1.1382e-02,  3.3242e-01,  3.3903e-01, -1.3554e-01,\n",
      "          5.4456e-01,  4.4522e-01,  6.3419e-01, -3.4453e-01, -9.2752e-02,\n",
      "          6.9279e-01,  3.0567e-01,  2.1444e-01,  5.7113e-01,  4.2851e-01,\n",
      "         -2.2044e-01, -5.8587e-02,  9.1116e-02, -1.2846e-01, -2.3586e-02,\n",
      "         -3.0925e-01,  3.9988e-01, -5.9166e-01,  3.4533e-01,  6.3146e-01,\n",
      "          2.7231e-01,  8.6103e-02,  2.4459e-02,  3.4568e-02,  6.7904e-01,\n",
      "         -1.4415e-01,  2.9280e-01, -3.7655e-01,  4.4416e-01,  3.6289e-02,\n",
      "         -6.6389e-01, -7.2362e-02, -1.5924e-01, -1.3359e-01, -6.3921e-01,\n",
      "          4.1814e-01,  2.2944e-01,  1.7157e-01, -1.9783e-01, -1.7085e-01,\n",
      "          1.1734e+00, -8.0776e-01,  4.1803e-01,  2.7606e-01,  4.3854e-01,\n",
      "          2.6709e-01, -3.0140e-01,  1.0290e-01, -4.7308e-03, -8.2638e-03,\n",
      "         -9.1004e-03, -1.7956e-01,  3.1310e-01, -2.2996e-01,  2.2280e-01,\n",
      "          2.9309e-01,  1.1278e-01,  1.9906e-01,  1.9155e-02,  3.9424e-03,\n",
      "          2.2293e-01,  7.3030e-01,  1.2634e-01, -7.8619e-01, -4.7858e-02,\n",
      "         -6.2938e-02, -1.9350e-01,  1.7715e-01,  1.7359e-01,  5.1319e-01,\n",
      "         -1.2499e-02,  3.8723e-01,  1.4369e-01, -2.2726e-02, -6.6127e-02,\n",
      "         -3.7771e-01,  2.9756e-01, -1.1447e-01,  2.4678e-01, -2.1517e-01,\n",
      "          7.9840e-01,  2.0638e-01,  5.5551e-01, -3.8652e-01,  3.3854e-01,\n",
      "         -1.6191e-01,  1.2834e-01, -2.9623e-01,  2.6478e-01,  2.4374e-01,\n",
      "          9.2520e-03, -2.8140e-01, -2.3010e+01, -1.3511e-01, -5.4281e-01,\n",
      "         -5.8497e-01, -3.2579e-01, -5.0077e-02,  2.6295e-01, -1.0869e-01,\n",
      "          6.3326e-02, -1.0859e-01,  1.2177e-01, -1.7163e-01, -4.4630e-02,\n",
      "          2.1996e-01, -2.7563e-01,  3.1477e-01, -9.1458e-01, -6.1674e-01,\n",
      "         -3.4145e-01,  1.7844e-01,  2.7581e-01,  6.6420e-01, -3.0873e-01,\n",
      "          6.9101e-01,  3.4750e-01,  8.6065e-01,  2.2182e-01, -3.4222e-02,\n",
      "          2.1508e-01,  3.8728e-01,  6.1111e-01,  5.8962e-02, -4.6439e-01,\n",
      "          1.5685e-01, -1.7588e-01,  1.2782e-01, -4.6638e-01, -1.6258e-01,\n",
      "          7.0844e-02,  5.0318e-02, -1.3497e-01, -1.0528e-01,  1.4680e-01,\n",
      "         -3.9061e-01, -1.1952e-02,  5.0782e-01, -4.7937e-01,  1.4558e-01,\n",
      "         -6.1128e-01,  1.5055e-01, -2.6357e-01,  1.8025e-01, -6.5256e-02,\n",
      "         -5.0307e-02, -8.9480e-02, -1.5452e-02,  2.4631e-02,  4.3775e-01,\n",
      "          4.7035e-01, -6.5574e-01,  2.9815e-01, -7.4580e-01,  4.6369e-01,\n",
      "          5.6504e-01, -1.4429e-01,  4.4687e-01,  1.0702e-01,  5.3058e-02,\n",
      "         -1.1236e-01, -4.2230e-01, -2.5846e-01,  4.4145e-02,  4.1907e-01,\n",
      "         -2.4996e-01,  8.1613e-02, -8.1775e-02,  3.3040e-02,  5.1549e-03,\n",
      "          1.9733e-01,  2.8023e-01, -1.0589e+00, -4.2103e-01,  3.5984e-01,\n",
      "         -3.4554e-01,  3.5647e-01,  1.2450e-02, -2.0691e-01,  4.8330e-01,\n",
      "         -1.5786e-01,  2.8412e-02,  2.1252e-01,  1.1473e-01,  1.5590e-01,\n",
      "         -3.5009e-01, -4.1818e-02, -2.5257e-01, -4.7378e-01, -2.2067e-01,\n",
      "         -3.0005e-01,  1.6934e-01, -2.2405e-02,  4.9223e-01,  4.0397e-01,\n",
      "          1.0168e-01,  2.3835e-01,  4.9413e-02, -1.2320e-02, -3.2354e-02,\n",
      "         -2.6082e-01, -1.8178e-01, -1.9515e-01, -2.2762e-01, -1.4025e-01,\n",
      "          1.0248e-01, -7.8868e-02,  3.5891e-01,  3.1249e-01, -8.9567e-02,\n",
      "         -4.1516e-01, -1.9992e-01, -6.4156e-01, -1.5183e-01,  1.7396e-01,\n",
      "         -9.8002e-02, -2.5444e-01,  1.1922e-01,  7.2508e-02,  1.5191e-01,\n",
      "          1.3480e-02,  2.9699e-01,  2.2466e-01,  2.3064e-01, -4.5594e-01,\n",
      "          3.0193e-01, -4.8366e-01,  4.1988e-02,  1.1258e-02,  3.1121e-01,\n",
      "         -4.0468e-01, -2.8346e-01, -1.2363e+00,  1.0630e-01,  7.8019e-01,\n",
      "         -1.8199e-02,  3.4527e-01, -3.9335e-01, -8.0980e-02, -2.6489e-01,\n",
      "          2.7832e-01,  4.4773e-01, -9.1212e-02, -9.7735e-02,  4.0849e-01,\n",
      "         -4.1835e-01, -3.5125e-01, -3.5796e-01, -7.8360e-02,  1.5134e-01,\n",
      "         -2.5780e-01, -3.2832e-01, -3.2688e-01, -5.1714e-01, -1.3030e-01,\n",
      "          2.1724e-01, -2.4192e-01,  2.9653e-01, -1.0372e-01, -7.9359e-01,\n",
      "         -2.7870e-01,  1.9327e-01, -2.9109e-01, -2.9879e-01, -3.1220e-02,\n",
      "         -5.1600e-01, -2.5115e-01,  1.1885e-01,  3.0354e-01, -5.0496e-02,\n",
      "          5.3399e-01,  2.6374e-01, -2.2413e-01, -9.6527e-02, -4.7633e-01,\n",
      "          1.1155e-01,  3.3344e-03,  1.3588e-01,  3.2734e-01,  1.5893e-01,\n",
      "         -1.1734e-01, -3.3918e-01,  4.5944e-01, -2.9349e-01, -2.5619e-01,\n",
      "         -1.2407e-01,  2.5891e-01,  6.3434e-01,  1.2800e-03, -6.3191e-02,\n",
      "          3.8685e-02, -3.9385e-01, -3.2986e-01, -1.8126e-01,  5.9702e-02,\n",
      "         -4.2360e-01, -4.4899e-01,  1.0717e-01, -3.7011e-01,  3.8038e-01,\n",
      "         -4.1986e-01,  2.4461e-01,  1.0323e+00]]), 'user': tensor([[-0.0111, -0.0716, -0.1016,  ...,  0.4221, -0.1941,  0.2685],\n",
      "        [ 0.0332,  0.1159,  0.0657,  ...,  0.3730, -0.0025,  0.5887],\n",
      "        [ 0.0510,  0.1516, -0.0072,  ...,  0.4285, -0.0285,  0.5689],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0708,  0.0217,  ...,  0.3643, -0.0376,  0.5008],\n",
      "        [ 0.0360,  0.0502,  0.0446,  ...,  0.4044, -0.0616,  0.4972],\n",
      "        [ 0.0360,  0.0502,  0.0446,  ...,  0.4044, -0.0616,  0.4972]])}\n",
      "Source-to-user edges: torch.Size([2, 27])\n",
      "User-to-user edges: torch.Size([2, 44])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import UPFD\n",
    "\n",
    "# Load the homogeneous dataset\n",
    "dataset = UPFD('data/upfd', name=\"politifact\", feature='bert', split=\"train\")\n",
    "\n",
    "# Convert to heterogeneous graph\n",
    "hetero_graph = convert_single_graph(dataset[0])\n",
    "\n",
    "# Print the heterogeneous graph structure\n",
    "print(hetero_graph)\n",
    "print(\"Node types:\", hetero_graph.node_types)\n",
    "print(\"Edge types:\", hetero_graph.edge_types)\n",
    "print(\"Source features shape:\", hetero_graph['source'].x.shape)\n",
    "print(\"User features shape:\", hetero_graph['user'].x.shape)\n",
    "print(\"node feature dict\", hetero_graph.x_dict)\n",
    "if ('source', 'to', 'user') in hetero_graph.edge_types:\n",
    "    print(\"Source-to-user edges:\", hetero_graph['source', 'to', 'user'].edge_index.shape)\n",
    "if ('user', 'to', 'user') in hetero_graph.edge_types:\n",
    "    print(\"User-to-user edges:\", hetero_graph['user', 'to', 'user'].edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032485c",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d8e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import IMDB\n",
    "from torch_geometric.nn import HANConv\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, in_channels: Union[int, Dict[str, int]],\n",
    "                 out_channels: int, hidden_channels=128, heads=8, metadata=None, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.han_conv = HANConv(in_channels, hidden_channels, heads=heads,\n",
    "                                dropout=dropout, metadata=metadata)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x = self.han_conv(x_dict, edge_index_dict)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class HANForGraphClassification(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=128, heads=8, metadata=None,\n",
    "                 dropout=0.6, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.han_conv = HANConv(in_channels, hidden_channels, heads=heads,\n",
    "                               dropout=dropout, metadata=metadata)\n",
    "        \n",
    "        # Linear layer will be initialized during forward pass once we know the input dimension\n",
    "        self.lin = None\n",
    "        self.out_channels = out_channels\n",
    "        self.classifier = nn.Linear(out_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Get node embeddings from HANConv\n",
    "        node_embeddings_dict = self.han_conv(x_dict, edge_index_dict)\n",
    "        \n",
    "        # Average pooling for each node type\n",
    "        pooled_embeddings = []\n",
    "        \n",
    "        for node_type, embeddings in node_embeddings_dict.items():\n",
    "            if embeddings is not None:\n",
    "                # Average pooling for nodes of the same type\n",
    "                pooled = torch.mean(embeddings, dim=0)\n",
    "                pooled_embeddings.append(pooled)\n",
    "        \n",
    "        if not pooled_embeddings:\n",
    "            raise ValueError(\"No node embeddings were produced by the model\")\n",
    "        \n",
    "        # Concatenate all pooled embeddings from different node types\n",
    "        x = torch.cat(pooled_embeddings)\n",
    "        \n",
    "        # Initialize the linear layer if not done yet\n",
    "        if self.lin is None:\n",
    "            lin_input_dim = x.size(0)\n",
    "            self.lin = nn.Linear(lin_input_dim, self.out_channels).to(x.device)\n",
    "        \n",
    "        # Apply linear layer and classifier\n",
    "        x = self.lin(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd41b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  source={\n",
      "    x=[1, 768],\n",
      "    y=[1],\n",
      "  },\n",
      "  user={ x=[71, 768] },\n",
      "  (source, to, user)={ edge_index=[2, 27] },\n",
      "  (user, to, user)={ edge_index=[2, 44] },\n",
      "  (source, to, source)={ edge_index=[2, 1] }\n",
      ")\n",
      "{'source': tensor([[ 4.9833e-01,  1.4148e-01,  3.5605e-01, -4.5109e-01, -3.0611e-01,\n",
      "          2.5061e-02,  2.8002e-01, -5.6461e-02, -2.2193e-01, -3.1098e-01,\n",
      "         -3.6787e-01, -5.0690e-02,  9.9056e-02, -2.7402e-01, -2.9179e-01,\n",
      "         -1.3563e-01,  1.1535e-01,  3.4642e-02, -2.8102e-01,  4.0781e-02,\n",
      "         -2.3397e-01, -1.9931e-01,  3.7839e-01, -9.2718e-02,  1.5422e-01,\n",
      "         -1.0526e-01,  7.9626e-02,  2.6651e-02,  3.3124e-01,  2.2782e-01,\n",
      "         -4.6196e-01, -9.3723e-02, -2.3510e-01, -1.8293e-02, -5.5263e-01,\n",
      "         -2.1882e-01,  1.3237e-01,  2.8989e-01, -7.3877e-02, -3.4337e-01,\n",
      "          2.2737e-01,  1.5777e-01, -3.2284e-01, -1.1594e-01, -1.7053e-01,\n",
      "         -3.4415e-01,  4.1308e-01,  5.5251e-01,  4.9701e-02,  2.2024e-01,\n",
      "          6.8173e-01, -1.8886e-01,  2.6792e-01, -3.4116e-01,  6.0337e-02,\n",
      "         -1.7919e-01, -1.0741e-01,  3.4089e-01,  2.6955e-01,  4.6229e-01,\n",
      "         -8.8313e-02, -1.7848e-01,  1.6680e-01, -1.6757e-02, -4.4666e-01,\n",
      "          1.0592e-01, -3.8689e-01, -8.7399e-02,  1.9867e-01, -3.6734e-01,\n",
      "         -2.7909e-02, -5.9345e-02, -2.4496e-01, -2.0377e-02, -1.2686e-01,\n",
      "         -1.9084e-01,  3.7032e-01, -3.9597e-01, -5.1814e-01, -1.3733e-01,\n",
      "          3.8600e-01,  1.5448e-01,  4.9865e-01,  5.7970e-01, -2.2777e-01,\n",
      "         -1.4674e-01,  3.4907e-01, -3.0398e-01,  3.8743e-01,  4.6746e-01,\n",
      "          1.2006e-02, -2.3832e-01,  1.5921e-02, -2.0881e-02,  1.3359e-01,\n",
      "          1.4889e-02,  1.2235e-02, -3.4089e-01, -7.7444e-01,  8.4228e-02,\n",
      "          1.8639e-01, -1.2925e-01,  1.4810e-01,  1.3271e-01,  3.9458e-03,\n",
      "          1.1254e-01,  3.2817e-01, -4.1709e-01, -3.7924e-01, -1.4918e-01,\n",
      "         -2.6106e-01,  2.4664e-01,  2.3349e-01,  1.8836e-01,  5.8085e-01,\n",
      "          2.6960e-01, -2.5443e-01,  2.6301e-01,  9.3538e-02,  4.4720e-01,\n",
      "          4.3324e-02, -1.9146e-01,  1.2773e-01, -2.6602e-02,  6.6994e-01,\n",
      "         -1.1637e-01,  1.7130e-01,  1.1850e-02,  5.2565e-01,  1.6589e-01,\n",
      "         -2.8751e-01,  1.4336e-01,  1.7929e-01, -3.1694e-01,  6.4360e-01,\n",
      "          2.4028e-01,  2.0684e-01,  4.9159e-01, -4.0610e-01, -1.0112e-01,\n",
      "         -2.0397e-01, -1.2868e-01,  6.9020e-02, -1.6788e-01, -1.1398e-01,\n",
      "         -9.6432e-02, -1.3847e-01, -5.1862e-02,  3.6143e-01, -2.2389e-01,\n",
      "         -2.8393e-01, -4.2352e-01,  9.0222e-02, -2.0520e-01, -7.9617e-02,\n",
      "         -2.7483e-01, -2.1710e-01, -8.3181e-02,  3.5027e-01, -4.1755e-01,\n",
      "          2.5407e-01, -1.8569e-01, -7.4766e-02, -5.8946e-01,  5.8267e-02,\n",
      "          1.7584e-02,  3.4543e-01, -9.0783e-02,  2.4536e-01,  3.0861e-01,\n",
      "          2.8589e-01,  1.4864e-01, -5.8605e-01,  6.1834e-01,  3.9532e-02,\n",
      "         -4.5143e-02,  7.2293e-01, -2.1009e-01, -1.4700e-01,  7.8769e-02,\n",
      "         -5.0227e-01,  2.1308e-01,  9.9657e-02,  1.5039e-01,  3.3213e-01,\n",
      "         -8.2904e-03, -7.5821e-01, -2.8153e-01, -1.0798e-01, -1.0624e-01,\n",
      "         -3.1508e-02,  4.7309e-01,  4.3489e-02, -3.2774e-01, -3.9268e-01,\n",
      "         -4.0245e-01,  8.6964e-01,  6.3158e-01, -7.7972e-02, -6.4270e-01,\n",
      "         -3.1103e-01,  2.1047e-01,  3.5432e-01,  8.0300e-02, -3.3451e-01,\n",
      "          9.8429e-03, -4.0907e-02, -9.2478e-02,  1.8670e-01, -2.2843e-01,\n",
      "         -2.2936e-01, -8.7489e-01, -4.4976e-01,  8.7495e-02, -4.7738e-01,\n",
      "         -4.2775e-01, -2.0363e-01, -6.2712e-01,  2.3708e-01,  4.2794e-01,\n",
      "         -1.7671e-01,  2.0321e-01, -1.6231e-02, -5.1056e-01, -1.7538e-01,\n",
      "          2.5584e-01, -5.8265e-02, -1.6288e-02,  5.8066e-01, -3.5122e-01,\n",
      "          2.8434e-01,  9.1404e-01, -1.5739e-01,  2.5894e-02,  2.4057e-01,\n",
      "         -4.1128e-01,  3.0834e-01, -2.7997e-01, -5.7539e-02,  3.1631e-01,\n",
      "         -2.9495e-01,  1.2284e-01,  3.1693e-01, -2.8447e-01,  1.2823e-02,\n",
      "          4.5972e-01, -2.2120e-01,  2.3481e-01, -4.1863e-01, -4.1139e-01,\n",
      "          5.2463e-01,  5.3397e-01, -3.5903e-01, -8.9240e-02,  1.6699e-02,\n",
      "          9.7468e-02,  1.4856e-01,  3.3786e-01,  1.9957e-02, -3.5565e-01,\n",
      "          6.9581e-01, -2.9245e-01, -3.0304e-02, -2.1578e-02, -3.4737e-01,\n",
      "         -9.6716e-01, -3.1507e-02,  2.0279e-01,  3.6645e-02, -2.2937e-01,\n",
      "         -9.8020e-02,  1.2358e-01,  3.3672e-01, -1.3031e-01,  3.0900e-01,\n",
      "          2.2685e-01,  1.7055e-01,  1.7900e-01,  1.2701e-01, -4.6329e-01,\n",
      "          3.3971e-01, -5.0062e-01, -4.4763e-01,  4.4535e-01, -4.7973e-01,\n",
      "         -1.2834e-01, -2.7894e-01,  5.9853e-03,  2.1849e-01,  4.3268e-02,\n",
      "         -1.8651e-01,  2.7100e-01,  3.5050e-01, -1.2949e-01,  9.0714e-02,\n",
      "          7.3053e-02, -7.3582e-01, -1.2262e-02, -4.3288e-01,  8.0753e-02,\n",
      "          2.9504e-01, -2.2287e-01,  4.9008e-01,  3.7230e-01,  3.0273e-01,\n",
      "         -1.1493e-01,  9.6005e-02,  5.0240e-01,  4.2250e-01,  1.5269e-01,\n",
      "          6.5372e-01,  1.1577e-01, -1.5920e-01,  5.1635e-01, -9.6676e-02,\n",
      "         -3.3659e-01, -2.6446e-02, -5.7905e-02, -2.5754e-01,  1.2444e-01,\n",
      "          1.7474e-01, -2.0362e-01, -3.6402e-01,  5.5095e-01,  6.5323e-02,\n",
      "          4.4618e-01, -4.7662e-01,  2.3480e-02,  5.2280e-02,  2.2481e-01,\n",
      "         -2.4435e-01,  3.8269e-02,  5.5869e-01, -3.8977e-01,  2.0469e-01,\n",
      "          5.0389e-02,  3.3900e-01,  8.1077e-02, -1.6302e-01,  6.7280e-02,\n",
      "          4.6966e-01,  2.3239e-01, -1.0848e-01, -3.2918e-01,  5.8506e-01,\n",
      "          4.2029e-04, -1.1313e-02, -4.0298e-01, -4.5129e-01, -4.1209e-01,\n",
      "         -1.2474e+00, -8.1422e-01, -4.4912e-01,  7.3039e-01,  6.2829e-01,\n",
      "          3.1743e-02, -8.8656e-02,  9.3864e-02,  1.2027e-01,  4.2917e-02,\n",
      "          1.2135e-01, -7.7527e-02,  1.0793e-01, -3.6876e-02,  2.0689e-01,\n",
      "         -7.4890e-02,  5.9522e-02,  6.0239e-01, -8.0083e-02, -4.3983e-01,\n",
      "          6.4783e-01, -2.9898e-01,  1.8574e-01,  2.8676e-01,  1.1925e-01,\n",
      "         -1.3693e-01, -6.5880e-02,  2.9003e-01,  4.8045e-01, -2.7925e-01,\n",
      "          1.4648e-04,  2.2738e-01,  1.1611e-01, -1.1670e-01, -2.0747e-01,\n",
      "          4.2796e-01,  6.5658e-02, -5.7038e-01,  3.0141e-01, -2.4589e-01,\n",
      "         -1.2180e-01,  2.4510e-01, -2.8302e-02, -4.6276e-01,  2.2287e-01,\n",
      "         -2.6033e-01, -2.3545e-01,  1.4743e-01, -2.6282e-01,  1.1634e-01,\n",
      "         -5.6174e-01,  7.1446e-02,  4.5546e-02,  1.2560e-02, -7.6813e-01,\n",
      "         -9.7880e-02, -2.2545e-01,  6.0543e-02, -9.9291e-02,  3.1972e-01,\n",
      "         -4.4419e-01, -6.4069e-01,  3.3769e-01,  2.0813e-01,  8.7301e-01,\n",
      "         -4.7873e-01, -3.9192e-01,  1.9400e-01, -2.6025e-01,  3.6889e-02,\n",
      "          3.0272e-01,  3.1199e-01,  1.2644e-01, -4.2758e-02, -2.1478e-02,\n",
      "         -1.1296e-01, -3.1382e-01, -6.3816e-01, -9.2857e-02, -2.2426e-02,\n",
      "          1.4191e-01, -4.6071e-01,  6.1619e-01,  3.2170e-01, -5.0485e-01,\n",
      "         -1.9337e-01,  7.7050e-01, -2.8134e-01,  1.7639e-01, -4.2143e-01,\n",
      "          8.1746e-03, -5.2912e-02,  5.9354e-01, -4.3711e-01,  4.3316e-01,\n",
      "         -9.6088e-02,  9.9395e-02, -5.5046e-02,  6.8160e-02, -3.4798e-01,\n",
      "          3.8596e-01, -8.1237e-02,  2.0905e-01,  2.0193e-01,  7.0137e-02,\n",
      "         -1.1661e+00, -4.7570e-01,  2.2337e-01, -3.7898e-01, -5.3860e-02,\n",
      "         -1.6614e-01, -1.1382e-02,  3.3242e-01,  3.3903e-01, -1.3554e-01,\n",
      "          5.4456e-01,  4.4522e-01,  6.3419e-01, -3.4453e-01, -9.2752e-02,\n",
      "          6.9279e-01,  3.0567e-01,  2.1444e-01,  5.7113e-01,  4.2851e-01,\n",
      "         -2.2044e-01, -5.8587e-02,  9.1116e-02, -1.2846e-01, -2.3586e-02,\n",
      "         -3.0925e-01,  3.9988e-01, -5.9166e-01,  3.4533e-01,  6.3146e-01,\n",
      "          2.7231e-01,  8.6103e-02,  2.4459e-02,  3.4568e-02,  6.7904e-01,\n",
      "         -1.4415e-01,  2.9280e-01, -3.7655e-01,  4.4416e-01,  3.6289e-02,\n",
      "         -6.6389e-01, -7.2362e-02, -1.5924e-01, -1.3359e-01, -6.3921e-01,\n",
      "          4.1814e-01,  2.2944e-01,  1.7157e-01, -1.9783e-01, -1.7085e-01,\n",
      "          1.1734e+00, -8.0776e-01,  4.1803e-01,  2.7606e-01,  4.3854e-01,\n",
      "          2.6709e-01, -3.0140e-01,  1.0290e-01, -4.7308e-03, -8.2638e-03,\n",
      "         -9.1004e-03, -1.7956e-01,  3.1310e-01, -2.2996e-01,  2.2280e-01,\n",
      "          2.9309e-01,  1.1278e-01,  1.9906e-01,  1.9155e-02,  3.9424e-03,\n",
      "          2.2293e-01,  7.3030e-01,  1.2634e-01, -7.8619e-01, -4.7858e-02,\n",
      "         -6.2938e-02, -1.9350e-01,  1.7715e-01,  1.7359e-01,  5.1319e-01,\n",
      "         -1.2499e-02,  3.8723e-01,  1.4369e-01, -2.2726e-02, -6.6127e-02,\n",
      "         -3.7771e-01,  2.9756e-01, -1.1447e-01,  2.4678e-01, -2.1517e-01,\n",
      "          7.9840e-01,  2.0638e-01,  5.5551e-01, -3.8652e-01,  3.3854e-01,\n",
      "         -1.6191e-01,  1.2834e-01, -2.9623e-01,  2.6478e-01,  2.4374e-01,\n",
      "          9.2520e-03, -2.8140e-01, -2.3010e+01, -1.3511e-01, -5.4281e-01,\n",
      "         -5.8497e-01, -3.2579e-01, -5.0077e-02,  2.6295e-01, -1.0869e-01,\n",
      "          6.3326e-02, -1.0859e-01,  1.2177e-01, -1.7163e-01, -4.4630e-02,\n",
      "          2.1996e-01, -2.7563e-01,  3.1477e-01, -9.1458e-01, -6.1674e-01,\n",
      "         -3.4145e-01,  1.7844e-01,  2.7581e-01,  6.6420e-01, -3.0873e-01,\n",
      "          6.9101e-01,  3.4750e-01,  8.6065e-01,  2.2182e-01, -3.4222e-02,\n",
      "          2.1508e-01,  3.8728e-01,  6.1111e-01,  5.8962e-02, -4.6439e-01,\n",
      "          1.5685e-01, -1.7588e-01,  1.2782e-01, -4.6638e-01, -1.6258e-01,\n",
      "          7.0844e-02,  5.0318e-02, -1.3497e-01, -1.0528e-01,  1.4680e-01,\n",
      "         -3.9061e-01, -1.1952e-02,  5.0782e-01, -4.7937e-01,  1.4558e-01,\n",
      "         -6.1128e-01,  1.5055e-01, -2.6357e-01,  1.8025e-01, -6.5256e-02,\n",
      "         -5.0307e-02, -8.9480e-02, -1.5452e-02,  2.4631e-02,  4.3775e-01,\n",
      "          4.7035e-01, -6.5574e-01,  2.9815e-01, -7.4580e-01,  4.6369e-01,\n",
      "          5.6504e-01, -1.4429e-01,  4.4687e-01,  1.0702e-01,  5.3058e-02,\n",
      "         -1.1236e-01, -4.2230e-01, -2.5846e-01,  4.4145e-02,  4.1907e-01,\n",
      "         -2.4996e-01,  8.1613e-02, -8.1775e-02,  3.3040e-02,  5.1549e-03,\n",
      "          1.9733e-01,  2.8023e-01, -1.0589e+00, -4.2103e-01,  3.5984e-01,\n",
      "         -3.4554e-01,  3.5647e-01,  1.2450e-02, -2.0691e-01,  4.8330e-01,\n",
      "         -1.5786e-01,  2.8412e-02,  2.1252e-01,  1.1473e-01,  1.5590e-01,\n",
      "         -3.5009e-01, -4.1818e-02, -2.5257e-01, -4.7378e-01, -2.2067e-01,\n",
      "         -3.0005e-01,  1.6934e-01, -2.2405e-02,  4.9223e-01,  4.0397e-01,\n",
      "          1.0168e-01,  2.3835e-01,  4.9413e-02, -1.2320e-02, -3.2354e-02,\n",
      "         -2.6082e-01, -1.8178e-01, -1.9515e-01, -2.2762e-01, -1.4025e-01,\n",
      "          1.0248e-01, -7.8868e-02,  3.5891e-01,  3.1249e-01, -8.9567e-02,\n",
      "         -4.1516e-01, -1.9992e-01, -6.4156e-01, -1.5183e-01,  1.7396e-01,\n",
      "         -9.8002e-02, -2.5444e-01,  1.1922e-01,  7.2508e-02,  1.5191e-01,\n",
      "          1.3480e-02,  2.9699e-01,  2.2466e-01,  2.3064e-01, -4.5594e-01,\n",
      "          3.0193e-01, -4.8366e-01,  4.1988e-02,  1.1258e-02,  3.1121e-01,\n",
      "         -4.0468e-01, -2.8346e-01, -1.2363e+00,  1.0630e-01,  7.8019e-01,\n",
      "         -1.8199e-02,  3.4527e-01, -3.9335e-01, -8.0980e-02, -2.6489e-01,\n",
      "          2.7832e-01,  4.4773e-01, -9.1212e-02, -9.7735e-02,  4.0849e-01,\n",
      "         -4.1835e-01, -3.5125e-01, -3.5796e-01, -7.8360e-02,  1.5134e-01,\n",
      "         -2.5780e-01, -3.2832e-01, -3.2688e-01, -5.1714e-01, -1.3030e-01,\n",
      "          2.1724e-01, -2.4192e-01,  2.9653e-01, -1.0372e-01, -7.9359e-01,\n",
      "         -2.7870e-01,  1.9327e-01, -2.9109e-01, -2.9879e-01, -3.1220e-02,\n",
      "         -5.1600e-01, -2.5115e-01,  1.1885e-01,  3.0354e-01, -5.0496e-02,\n",
      "          5.3399e-01,  2.6374e-01, -2.2413e-01, -9.6527e-02, -4.7633e-01,\n",
      "          1.1155e-01,  3.3344e-03,  1.3588e-01,  3.2734e-01,  1.5893e-01,\n",
      "         -1.1734e-01, -3.3918e-01,  4.5944e-01, -2.9349e-01, -2.5619e-01,\n",
      "         -1.2407e-01,  2.5891e-01,  6.3434e-01,  1.2800e-03, -6.3191e-02,\n",
      "          3.8685e-02, -3.9385e-01, -3.2986e-01, -1.8126e-01,  5.9702e-02,\n",
      "         -4.2360e-01, -4.4899e-01,  1.0717e-01, -3.7011e-01,  3.8038e-01,\n",
      "         -4.1986e-01,  2.4461e-01,  1.0323e+00]]), 'user': tensor([[-0.0111, -0.0716, -0.1016,  ...,  0.4221, -0.1941,  0.2685],\n",
      "        [ 0.0332,  0.1159,  0.0657,  ...,  0.3730, -0.0025,  0.5887],\n",
      "        [ 0.0510,  0.1516, -0.0072,  ...,  0.4285, -0.0285,  0.5689],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0708,  0.0217,  ...,  0.3643, -0.0376,  0.5008],\n",
      "        [ 0.0360,  0.0502,  0.0446,  ...,  0.4044, -0.0616,  0.4972],\n",
      "        [ 0.0360,  0.0502,  0.0446,  ...,  0.4044, -0.0616,  0.4972]])}\n",
      "{('source', 'to', 'user'): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26]]), ('user', 'to', 'user'): tensor([[ 7,  7,  7, 15, 15, 15, 15, 15, 15, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 26, 26, 26, 26, 26,\n",
      "         26, 26, 26, 26, 26, 26, 26, 59],\n",
      "        [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
      "         45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n",
      "         63, 64, 65, 66, 67, 68, 69, 70]]), ('source', 'to', 'source'): tensor([[0],\n",
      "        [0]])}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dummy_graph = convert_single_graph(dataset[0], add_source_self_loop=True)\n",
    "print(dummy_graph)\n",
    "print(dummy_graph.x_dict)\n",
    "print(dummy_graph.edge_index_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17aa021",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81efd9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANForGraphClassification(\n",
      "  (han_conv): HANConv(128, heads=8)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "tensor([0.0107, 0.0673], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "han_model = HANForGraphClassification(\n",
    "    in_channels=dummy_graph.num_node_features,\n",
    "    out_channels=128,\n",
    "    hidden_channels=128,\n",
    "    heads=8,\n",
    "    metadata=dummy_graph.metadata(),\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "han_model = han_model.to(device)\n",
    "dummy_graph = dummy_graph.to(device)\n",
    "\n",
    "print(han_model)\n",
    "han_model.eval()\n",
    "out = han_model(dummy_graph.x_dict, dummy_graph.edge_index_dict)\n",
    "print(out)\n",
    "print(dummy_graph['source'].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "182cb6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([0], device='cuda:0')\n",
      "tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = han_model(dummy_graph.x_dict, dummy_graph.edge_index_dict)\n",
    "out = out.unsqueeze(dim=0)\n",
    "print(out.shape)\n",
    "print(dummy_graph['source'].y)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(out, dummy_graph['source'].y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b13a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as HeteroDataLoader\n",
    "\n",
    "# Load the original datasets\n",
    "train_dataset_orig = UPFD('data/upfd', name='politifact', feature='bert', split='train')\n",
    "val_dataset_orig = UPFD('data/upfd', name='politifact', feature='bert', split='val')\n",
    "test_dataset_orig = UPFD('data/upfd', name='politifact', feature='bert', split='test')\n",
    "\n",
    "# Convert to heterogeneous datasets\n",
    "train_dataset = convert_to_heterogeneous(train_dataset_orig, add_source_self_loop=True)\n",
    "val_dataset = convert_to_heterogeneous(val_dataset_orig, add_source_self_loop=True)\n",
    "test_dataset = convert_to_heterogeneous(test_dataset_orig, add_source_self_loop=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = HeteroDataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = HeteroDataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = HeteroDataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6cea42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss: 0.08084781177420679\n",
      "Validation Accuracy: 0.6452\n",
      "Epoch 1, Avg Loss: 0.0494933859090438\n",
      "Validation Accuracy: 0.7097\n",
      "Epoch 2, Avg Loss: 0.030985307638719723\n",
      "Validation Accuracy: 0.6774\n",
      "Epoch 3, Avg Loss: 0.02896637809786548\n",
      "Validation Accuracy: 0.6452\n",
      "Epoch 4, Avg Loss: 0.026702903495948133\n",
      "Validation Accuracy: 0.7097\n",
      "Epoch 5, Avg Loss: 0.0234403375196913\n",
      "Validation Accuracy: 0.6774\n",
      "Epoch 6, Avg Loss: 0.021567187815370654\n",
      "Validation Accuracy: 0.7097\n",
      "Epoch 7, Avg Loss: 0.0212852090640582\n",
      "Validation Accuracy: 0.7097\n",
      "Epoch 8, Avg Loss: 0.014492097865876564\n",
      "Validation Accuracy: 0.7097\n",
      "Epoch 9, Avg Loss: 0.018071358524210813\n",
      "Validation Accuracy: 0.7097\n",
      "Test Accuracy: 0.7964\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(han_model.parameters(), lr=0.005)\n",
    "num_epochs = 10\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients over\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    han_model.train()\n",
    "    han_model.to(device)\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out = han_model(batch.x_dict, batch.edge_index_dict)\n",
    "        out = out.unsqueeze(dim=0)\n",
    "        loss = criterion(out, batch['source'].y)\n",
    "        \n",
    "        # Normalize loss to account for accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update weights after accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(f\"Epoch {epoch}, Avg Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    # Validation\n",
    "    han_model.eval()\n",
    "    correct = 0\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = han_model(batch.x_dict, batch.edge_index_dict)\n",
    "            out = out.unsqueeze(dim=0)\n",
    "            \n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch['source'].y).sum().item()\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "# Test\n",
    "han_model.eval()\n",
    "correct = 0\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = han_model(batch.x_dict, batch.edge_index_dict)\n",
    "        out = out.unsqueeze(dim=0)\n",
    "        \n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch['source'].y).sum().item()\n",
    "test_acc = correct / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
